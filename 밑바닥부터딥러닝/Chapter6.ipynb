{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78d509d-5f52-4cc9-9136-8c39f616f24a",
   "metadata": {},
   "source": [
    "# 6. 학습 관련 기술들\n",
    "- 가중치 매개변수의 최적값을 탐색하는 최적화 방법\n",
    "- 가중치 매개변수 초깃값\n",
    "- 하이퍼파라미터 설정 방법\n",
    "- 가중치 감소, 드롭아웃\n",
    "- 배치 정규화\n",
    "- 위 방법들을 통한 효율과 정확도 상승"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a492f4c-71a9-4ff7-9bf6-c305d0fd774a",
   "metadata": {},
   "source": [
    "## 6.1 매개변수 갱신\n",
    "- 신경망 학습의 목적은 손실 함수의 값을 가능한 낮추는 매개변수를 찾는 것이다.\n",
    "- 이는 곧 매개변수의 최적값을 찾는 문제이며, 이러한 문제를 푸는 것을 최적화(optimization) 이라고 한다.\n",
    "- 매개변수 공간은 매우 넓고 복잡해서 최적의 솔루션은 쉽게 못 찾는다.\n",
    "- 지금까지는 최적화 방법으로 매개변수의 기울기를 구해, 기울어진 방향으로 방향으로 매개변수 값을 갱신하는 일을 반복해서 최적의 값에 다가갔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e2f73-e566-4e4e-aa2c-e65eaf8afbbc",
   "metadata": {},
   "source": [
    "### 6.1.2 확률적 경사 하강법(SGD)\n",
    "- 가중치들을 일정거리만큼(학습률) 기울어진 방향으로 이동하겠다는 단순한 방법이다.\n",
    "- 비효율적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50520e5-0c18-4e81-8da7-52fefc20dad2",
   "metadata": {},
   "source": [
    "### 6.1.4 모멘텀(momentum)\n",
    "- GD를 기반으로 한 optizizer이며, 이동하려는 방향으로 SGD보다 더 멀리 뻗는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5c40a-9464-4ee7-a2b7-e67c60c0c8c7",
   "metadata": {},
   "source": [
    "### 6.1.5 AdaGrad\n",
    "- 과거의 기울기를 제곱하여 계속 더해간다. 학습을 진행할 수록 갱신 강도가 약해지고 무한히 학습하게 되면 갱신량이 0이 되어 전혀 갱신이 안된다.\n",
    "- 이를 해결하기 위해 RMSProp 라는 방법이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe13fd4-302b-4843-9dda-880d381f7987",
   "metadata": {},
   "source": [
    "### 6.1.6 Adam\n",
    "- AdaGrad + Momentum 을 합친 방법 = Adam이다.\n",
    "- Adam 은 하이퍼파라미터를 3개 설정한다.\n",
    "- Adam 하이퍼파라미터 1. 학습률\n",
    "- Adam 하이퍼파라미터 2,3 일차 모멘텀용 계수, 이차 모멘텀용 계수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2234c-63b8-47eb-8a6a-ed7854684448",
   "metadata": {},
   "source": [
    "### 6.1.7 summary\n",
    "- Adam, AdaGrad 사용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ca75e-853b-4474-9ff6-5ad6df39f233",
   "metadata": {},
   "source": [
    "## 6.2 가중치의 초깃값\n",
    "- 가중치의 초깃값을 무엇으로 설정하느냐가 신경망 학습의 성패를 가르는 일이 실제로 자주 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253d9dd-0796-4f46-853c-c5a006b9308d",
   "metadata": {},
   "source": [
    "### 6.2.1 초깃값을 0으로 하면"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95365212-ffdd-494f-99ba-4601ed55f284",
   "metadata": {},
   "source": [
    "- 아주 똥같은 생각이다.\n",
    "- 가중치를 균일한 값으로 하는 그 자체가 안된다.\n",
    "- 가중치의 대칭적인 구조를 무너뜨리려면 초깃값을 무작위로 설정해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289449f-f760-4dac-804d-46c2c5c0860c",
   "metadata": {},
   "source": [
    "### summary\n",
    "- sigmoid, tanh 와 같은 경우 가중치의 초깃값을 Xavier 초깃값을 사용한다.\n",
    "- ReLU는 가중치를 HE 초깃값을 사용한다.(Xavier는 어느정도 선형인 것을 전제로 이끌기 때문이다. sigmoid, tanh는 선형은 아니지만 좌우가 대칭인 어느정도 선형인 함수로 볼 수 있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29005c2-e4ce-4063-91c1-db892f69782a",
   "metadata": {},
   "source": [
    "## 6.3 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3ae9a-27fe-4884-97e4-30aa919c51a3",
   "metadata": {},
   "source": [
    "- 각 층에서의 활성화 값 분포를 적당히 퍼트리며 학습이 월활하게 수행되게끔 가중치의 초깃값을 적절히 설정했다.\n",
    "- 각 층이 활성화함수 적용전 값들을 강제로 퍼트리는 방법 = 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd6cb0-203e-497a-8d7b-463591b3d9f4",
   "metadata": {},
   "source": [
    "### 6.3.1 배치 정규화 알고리즘\n",
    "- 학습 속도가 개선\n",
    "- 초깃값에 크게 의존하지 않는다\n",
    "- 오버피팅을 억제한다(drop out 필요성 감소)\n",
    "- 데이터(미니 배치 단위)의 분포가 평균이 0, 분산이 1이 되도록 정규화 한다.\n",
    "- 이를 이용해 초깃값에 크게 의존하지 않아도 된다는 장점이 생긴다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff272cf0-fe6b-46e6-96d2-3b8128509f96",
   "metadata": {},
   "source": [
    "## 6.4 바른 학습을 위해\n",
    "- 기계학습은 범용 성능을 지향한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc35ae6-395a-4ea4-a426-47d2788ce934",
   "metadata": {},
   "source": [
    "### 6.4.1 오버피팅\n",
    "- 매개변수가 많고 표현력이 높은 모델\n",
    "- 훈련 데이터가 적을 때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13faa596-522a-4532-979c-546bde5064a1",
   "metadata": {},
   "source": [
    "### 6.4.2 가중치 감소\n",
    "- 오버피팅 억제를 위해 사용하는 방법 중 가중치 감소 라는 것이 있다.\n",
    "- 학습과정에서 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과해 오버피팅을 억제하는 방법이다.\n",
    "- 오버피팅은 가중치 매개변수의 값이 커서 발생하는 경우가 많기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc0e48-713d-4f8b-be13-78637ad2f360",
   "metadata": {},
   "source": [
    "### 6.4.3 드롭 아웃\n",
    "- 신경망 모델이 복잡해지면 가중치 감소만으로 대응이 어렵다.\n",
    "- 뉴런을 임의로 삭제하면서 학습하는 방법\n",
    "- 기계학습에서 앙상블을 이용하는데, 신경망의 맥락에서 보면 같거나 비슷한 구조의 네트워크를 따로따로 학습시키고 그 평균을 낼 수 있다. 이러한 앙상블의 효과를 드롭아웃을 통해 얻어낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150892f-ef9d-417a-a99e-a6ba9ee40a90",
   "metadata": {},
   "source": [
    "## 6.5 적절한 하이퍼파라미터 값 찾기\n",
    "- 뉴런 수, 배치 크기, 매개수 갱신 시의 학습률, 가중치 감소 등이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e7e9f-2dd2-42c6-9ef4-3a14b55f2ea6",
   "metadata": {},
   "source": [
    "### 6.5.1 검증 데이터\n",
    "- 훈련용 데이터로 학습을 하고, 검증 데이터로 하이퍼 파라미터 값의 적합함을 판단하고, 시험용 데이터로 범용 성능을 평가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5529c0-6b13-4453-bf1d-6a82b89660d1",
   "metadata": {},
   "source": [
    "### 6.5.2 하이퍼 파라미터 최적화\n",
    "- 신경망의 하이퍼파라미터 최적화는 그리드 서치와 같은 규칙적인 탐색보다는 무작위 샘플링해 탐색하는 것이 좋다.\n",
    "- 딥러닝 학습은 오래걸리기 때문에 하이퍼파라미터 값들을 10의 거듭제곱 단위로, 에폭은 작게하여 평가하는것이 효과적이다.\n",
    "- 0단계 : 하이퍼 파라미터 값의 범위를 설정한다\n",
    "- 1단계 : 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출한다.\n",
    "- 2단계 : 샘플링한 하이퍼파리미터 값을 사용해 학습하고, 검증 데이터로 정확도를 평가한다.(epoch은 작게 설정)\n",
    "- 3단계 : 1~2단계를 특정 횟수 반복하면서 그 정확도를 보고 하이퍼파라미터의 범위를 좁혀간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1917c87-ed60-45f0-92d2-86cef63c214c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
