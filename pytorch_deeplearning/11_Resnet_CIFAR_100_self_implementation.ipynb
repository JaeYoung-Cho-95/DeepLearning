{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version : 1.7.1+cu110\n",
      "torch Device available : cuda\n",
      "train Batch_size : 64\n",
      "trani Epochs : 20\n"
     ]
    }
   ],
   "source": [
    "# 장비 할당하기\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "BATCH_SIZE = 64\n",
    "Epochs = 20\n",
    "\n",
    "print(\"torch version : {}\".format(torch.__version__))\n",
    "print(\"torch Device available : {}\".format(DEVICE))\n",
    "print(\"train Batch_size : {}\\ntrani Epochs : {}\".format(BATCH_SIZE,Epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR100 데이터 불러오기\n",
    "train_datasets = datasets.CIFAR100(root = './Data/',download = True, train = True, transform = transforms.ToTensor())\n",
    "test_datasets =  datasets.CIFAR100(root = './Data/',download = True, train = False,transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_datasets, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_datasets, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size : torch.Size([64, 3, 32, 32])\n",
      "label size : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# 지정한 배치사이즈로 내가 원하는 데이터가 불러와지는지 확인\n",
    "# 입력 차원 확인하기\n",
    "for (image, label) in train_loader:\n",
    "    print(\"image size : {}\".format(image.size()))\n",
    "    print(\"label size : {}\".format(label.size()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './images/resnet.png' width = 1000px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,in_planes,planes,stride = 1):\n",
    "        super(BasicBlock,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes,planes,kernel_size=3,stride=stride,padding=1,bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes,planes,kernel_size=3,stride=1,padding=1,bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # short_cut 의 역할은 stride가 달라 차원이 달라진 경우, kernel_size는 1로 고정하고 stride값을 조정해 차원을 맞춰주는 역할이다.\n",
    "        self.short_cut = nn.Sequential()\n",
    "        if stride != 1:\n",
    "            self.short_cut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes,planes,kernel_size=1,stride=stride, bias = False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.short_cut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes = 100):\n",
    "        super(ResNet18,self).__init__()\n",
    "        self.blocks = [2,2,2,2]\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3,64,kernel_size=3,stride=2,bias = False)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "\n",
    "        # layer0 의 stride 값이 1인 이유는 바로 위, MaxPool2d를 통해 stride값을 주며 이미 출력이미지에 맞춰서 넘어왔기 때문이다.\n",
    "        # 나머지 layer sequence 들은 다음 block으로 넘어갈 때 출력이미지가 절반으로 줄어들기 때문에 stride = 2 값을 준다.\n",
    "        self.layer1 = self.make_layer(64,self.blocks[0],stride=1)\n",
    "        self.layer2 = self.make_layer(128,self.blocks[1],stride=2)\n",
    "        self.layer3 = self.make_layer(256,self.blocks[2],stride=2)\n",
    "        self.layer4 = self.make_layer(512,self.blocks[3],stride=2)\n",
    "\n",
    "        self.fc_layer = nn.Linear(512,100)\n",
    "    \n",
    "    def make_layer(self,planes,block,stride):\n",
    "        strides = [stride] + block * [1]\n",
    "\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes,planes,stride))\n",
    "            self.in_planes = planes\n",
    "        \n",
    "        return nn.Sequential(* layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.pool1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        # Feature Map에 4*4 filter가 움직이면서 16개의 Feature Map 값의 평균을 계산해 1개의 Feature Map으로 다운샘플링\n",
    "        # out = F.avg_pool2d(out,8)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.fc_layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (fc_layer): Linear(in_features=512, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx,(image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch : {} [ {} / {} ( {:.0f}% ) ]'.format(Epoch, batch_idx * len(image), len(train_loader.dataset), 100 * batch_idx / len(train_loader)), end = ',  ')\n",
    "            print('Train Loss : {:.6f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output,label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy = 100 * correct / len(test_loader.dataset)\n",
    "\n",
    "        return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch : 1 [ 0 / 50000 ( 0% ) ],  Train Loss : 4.993938\n",
      "Train Epoch : 1 [ 12800 / 50000 ( 26% ) ],  Train Loss : 4.306392\n",
      "Train Epoch : 1 [ 25600 / 50000 ( 51% ) ],  Train Loss : 4.106871\n",
      "Train Epoch : 1 [ 38400 / 50000 ( 77% ) ],  Train Loss : 3.732512\n",
      "\n",
      "[Epoch : 1], \t Test Loss : 0.0620, \t Test Accuracy : 7.84 %\n",
      "\n",
      "Train Epoch : 2 [ 0 / 50000 ( 0% ) ],  Train Loss : 3.793797\n",
      "Train Epoch : 2 [ 12800 / 50000 ( 26% ) ],  Train Loss : 3.776159\n",
      "Train Epoch : 2 [ 25600 / 50000 ( 51% ) ],  Train Loss : 3.520288\n",
      "Train Epoch : 2 [ 38400 / 50000 ( 77% ) ],  Train Loss : 3.731096\n",
      "\n",
      "[Epoch : 2], \t Test Loss : 0.0592, \t Test Accuracy : 11.76 %\n",
      "\n",
      "Train Epoch : 3 [ 0 / 50000 ( 0% ) ],  Train Loss : 3.532098\n",
      "Train Epoch : 3 [ 12800 / 50000 ( 26% ) ],  Train Loss : 3.674908\n",
      "Train Epoch : 3 [ 25600 / 50000 ( 51% ) ],  Train Loss : 3.512073\n",
      "Train Epoch : 3 [ 38400 / 50000 ( 77% ) ],  Train Loss : 3.464995\n",
      "\n",
      "[Epoch : 3], \t Test Loss : 0.0548, \t Test Accuracy : 15.44 %\n",
      "\n",
      "Train Epoch : 4 [ 0 / 50000 ( 0% ) ],  Train Loss : 3.425390\n",
      "Train Epoch : 4 [ 12800 / 50000 ( 26% ) ],  Train Loss : 3.445445\n",
      "Train Epoch : 4 [ 25600 / 50000 ( 51% ) ],  Train Loss : 2.826277\n",
      "Train Epoch : 4 [ 38400 / 50000 ( 77% ) ],  Train Loss : 3.074705\n",
      "\n",
      "[Epoch : 4], \t Test Loss : 0.0479, \t Test Accuracy : 24.24 %\n",
      "\n",
      "Train Epoch : 5 [ 0 / 50000 ( 0% ) ],  Train Loss : 2.897983\n",
      "Train Epoch : 5 [ 12800 / 50000 ( 26% ) ],  Train Loss : 2.889573\n",
      "Train Epoch : 5 [ 25600 / 50000 ( 51% ) ],  Train Loss : 2.816717\n",
      "Train Epoch : 5 [ 38400 / 50000 ( 77% ) ],  Train Loss : 2.674730\n",
      "\n",
      "[Epoch : 5], \t Test Loss : 0.0561, \t Test Accuracy : 23.52 %\n",
      "\n",
      "Train Epoch : 6 [ 0 / 50000 ( 0% ) ],  Train Loss : 2.614373\n",
      "Train Epoch : 6 [ 12800 / 50000 ( 26% ) ],  Train Loss : 2.970923\n",
      "Train Epoch : 6 [ 25600 / 50000 ( 51% ) ],  Train Loss : 2.950247\n",
      "Train Epoch : 6 [ 38400 / 50000 ( 77% ) ],  Train Loss : 2.639161\n",
      "\n",
      "[Epoch : 6], \t Test Loss : 0.0435, \t Test Accuracy : 29.38 %\n",
      "\n",
      "Train Epoch : 7 [ 0 / 50000 ( 0% ) ],  Train Loss : 2.639765\n",
      "Train Epoch : 7 [ 12800 / 50000 ( 26% ) ],  Train Loss : 2.577286\n",
      "Train Epoch : 7 [ 25600 / 50000 ( 51% ) ],  Train Loss : 2.081604\n",
      "Train Epoch : 7 [ 38400 / 50000 ( 77% ) ],  Train Loss : 2.542398\n",
      "\n",
      "[Epoch : 7], \t Test Loss : 0.0443, \t Test Accuracy : 30.71 %\n",
      "\n",
      "Train Epoch : 8 [ 0 / 50000 ( 0% ) ],  Train Loss : 2.309757\n",
      "Train Epoch : 8 [ 12800 / 50000 ( 26% ) ],  Train Loss : 2.238542\n",
      "Train Epoch : 8 [ 25600 / 50000 ( 51% ) ],  Train Loss : 2.361878\n",
      "Train Epoch : 8 [ 38400 / 50000 ( 77% ) ],  Train Loss : 2.674148\n",
      "\n",
      "[Epoch : 8], \t Test Loss : 0.0419, \t Test Accuracy : 33.94 %\n",
      "\n",
      "Train Epoch : 9 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.906772\n",
      "Train Epoch : 9 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.974805\n",
      "Train Epoch : 9 [ 25600 / 50000 ( 51% ) ],  Train Loss : 2.382928\n",
      "Train Epoch : 9 [ 38400 / 50000 ( 77% ) ],  Train Loss : 2.306961\n",
      "\n",
      "[Epoch : 9], \t Test Loss : 0.0420, \t Test Accuracy : 34.07 %\n",
      "\n",
      "Train Epoch : 10 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.848046\n",
      "Train Epoch : 10 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.785648\n",
      "Train Epoch : 10 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.859148\n",
      "Train Epoch : 10 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.751865\n",
      "\n",
      "[Epoch : 10], \t Test Loss : 0.0435, \t Test Accuracy : 34.65 %\n",
      "\n",
      "Train Epoch : 11 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.791840\n",
      "Train Epoch : 11 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.989097\n",
      "Train Epoch : 11 [ 25600 / 50000 ( 51% ) ],  Train Loss : 2.221649\n",
      "Train Epoch : 11 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.731331\n",
      "\n",
      "[Epoch : 11], \t Test Loss : 0.0422, \t Test Accuracy : 36.42 %\n",
      "\n",
      "Train Epoch : 12 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.242749\n",
      "Train Epoch : 12 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.692009\n",
      "Train Epoch : 12 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.732250\n",
      "Train Epoch : 12 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.819916\n",
      "\n",
      "[Epoch : 12], \t Test Loss : 0.0440, \t Test Accuracy : 36.18 %\n",
      "\n",
      "Train Epoch : 13 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.770914\n",
      "Train Epoch : 13 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.524597\n",
      "Train Epoch : 13 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.200422\n",
      "Train Epoch : 13 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.312504\n",
      "\n",
      "[Epoch : 13], \t Test Loss : 0.0450, \t Test Accuracy : 36.97 %\n",
      "\n",
      "Train Epoch : 14 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.017609\n",
      "Train Epoch : 14 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.244736\n",
      "Train Epoch : 14 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.208207\n",
      "Train Epoch : 14 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.547009\n",
      "\n",
      "[Epoch : 14], \t Test Loss : 0.0474, \t Test Accuracy : 36.73 %\n",
      "\n",
      "Train Epoch : 15 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.873692\n",
      "Train Epoch : 15 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.675722\n",
      "Train Epoch : 15 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.901436\n",
      "Train Epoch : 15 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.940290\n",
      "\n",
      "[Epoch : 15], \t Test Loss : 0.0516, \t Test Accuracy : 34.22 %\n",
      "\n",
      "Train Epoch : 16 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.175497\n",
      "Train Epoch : 16 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.642231\n",
      "Train Epoch : 16 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.218105\n",
      "Train Epoch : 16 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.232312\n",
      "\n",
      "[Epoch : 16], \t Test Loss : 0.0519, \t Test Accuracy : 36.84 %\n",
      "\n",
      "Train Epoch : 17 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.819750\n",
      "Train Epoch : 17 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.992472\n",
      "Train Epoch : 17 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.689869\n",
      "Train Epoch : 17 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.068533\n",
      "\n",
      "[Epoch : 17], \t Test Loss : 0.0558, \t Test Accuracy : 36.29 %\n",
      "\n",
      "Train Epoch : 18 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.426262\n",
      "Train Epoch : 18 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.872920\n",
      "Train Epoch : 18 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.928198\n",
      "Train Epoch : 18 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.702138\n",
      "\n",
      "[Epoch : 18], \t Test Loss : 0.0562, \t Test Accuracy : 37.08 %\n",
      "\n",
      "Train Epoch : 19 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.454998\n",
      "Train Epoch : 19 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.275837\n",
      "Train Epoch : 19 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.447950\n",
      "Train Epoch : 19 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.848864\n",
      "\n",
      "[Epoch : 19], \t Test Loss : 0.0581, \t Test Accuracy : 37.04 %\n",
      "\n",
      "Train Epoch : 20 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.447017\n",
      "Train Epoch : 20 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.513281\n",
      "Train Epoch : 20 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.354162\n"
     ]
    }
   ],
   "source": [
    "for Epoch in range(1, Epochs+1):\n",
    "    train(model, train_loader, optimizer, 200)\n",
    "    test_loss, test_accracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[Epoch : {}], \\t Test Loss : {:.4f}, \\t Test Accuracy : {:.2f} %\\n\".format(Epoch, test_loss, test_accracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3a6afb4ca1356324ac719a8f4904d3d9af325fac862a66ccc73b4ecb2536197"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('torch1.7': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
