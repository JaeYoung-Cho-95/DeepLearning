{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "- 데이터를 임의로 변형해 데이터의 수를 늘려 다양한 Feature 를 뽑는 방법을 Data Augmentation 이라고 한다.\n",
    "#### (1) Flip\n",
    "- 이미지 반전\n",
    "#### (2) Rotation\n",
    "- 이미지 회전\n",
    "#### (3) Crop\n",
    "- 이미지 자르기\n",
    "#### (4) Scaling\n",
    "- 이미지 확대 축소\n",
    "#### (5) Cutout\n",
    "- 이미지 내부 일부를 사각형 모양으로 검은색을 칠하는 기법\n",
    "- 이미지에 Dropout 기법을 사용했다고 생각하면 됨\n",
    "#### (6) Cutmix \n",
    "- 두 이미지를 합쳐 놓고 Label을 학습시킬 때, 이미지가 차지하는 비율만큼 학습시키는 방법\n",
    "\n",
    "> Cutout, Cutmix 둘 다  일반적인 이미지 분류 문제에서 Data Augmentation 보다 성능이 뛰어나다는 것이 논문을 통해 밝혀짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version : 1.7.1+cu110\n",
      "torch device : cuda\n",
      "Batch size : 64\n",
      "Epochs : 10\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "BATCH_SIZE = 64\n",
    "Epochs = 10\n",
    "print('torch version : {}'.format(torch.__version__))\n",
    "print('torch device : {}'.format(DEVICE))\n",
    "print('Batch size : {}\\nEpochs : {}'.format(BATCH_SIZE, Epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_datasets = datasets.CIFAR10(root = './Data/CIFAR_10',\n",
    "                                  train=True,\n",
    "                                  download=True,\n",
    "                                  # ToTensor 가 아닌 Compose를 이용해 Augmentation 을 바로 진행할 수 있다.\n",
    "                                  transform = transforms.Compose([\n",
    "                                      # 50% 확률로 좌우반전\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      # 0에서 1사이의 값으로 정규화, Tensor의 형태로 변환\n",
    "                                      transforms.ToTensor(),\n",
    "                                      # 다시 한 번 정규화를 진행 red, green, blue 순으로 0.5의 평균, 표준편차로 적용\n",
    "                                      transforms.Normalize((0.5,0.5,0.5),\n",
    "                                      (0.5, 0.5, 0.5))\n",
    "                                  ]))\n",
    "test_datasets = datasets.CIFAR10(root = './Data/CIFAR_10',\n",
    "                                  train=False,\n",
    "                                  download=True,\n",
    "                                  transform = transforms.Compose([\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5),\n",
    "                                      (0.5, 0.5, 0.5))\n",
    "                                  ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_datasets, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_datasets, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train 의 size : torch.Size([64, 3, 32, 32])\n",
      "y_train 의 size : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for (x_train, y_train) in train_loader:\n",
    "    print(\"x_train 의 size : {}\".format(x_train.size()))\n",
    "    print(\"y_train 의 size : {}\".format(y_train.size()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1,8*8*16)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx,(image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch : {} [ {} / {} ( {:.0f}% ) ]'.format(Epoch, batch_idx * len(image), len(train_loader.dataset), 100 * batch_idx / len(train_loader)), end = ',  ')\n",
    "            print('Train Loss : {:.6f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output,label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy = 100 * correct / len(test_loader.dataset)\n",
    "\n",
    "        return test_loss, test_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAEYOU~1\\AppData\\Local\\Temp/ipykernel_6352/3664862933.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch : 1 [ 0 / 50000 ( 0% ) ],  Train Loss : 2.337559\n",
      "Train Epoch : 1 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.636152\n",
      "Train Epoch : 1 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.339103\n",
      "Train Epoch : 1 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.397903\n",
      "\n",
      "[Epoch : 1], \t Test Loss : 0.0220, \t Test Accuracy : 47.73 %\n",
      "\n",
      "Train Epoch : 2 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.297296\n",
      "Train Epoch : 2 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.326454\n",
      "Train Epoch : 2 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.229749\n",
      "Train Epoch : 2 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.329049\n",
      "\n",
      "[Epoch : 2], \t Test Loss : 0.0208, \t Test Accuracy : 53.64 %\n",
      "\n",
      "Train Epoch : 3 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.438379\n",
      "Train Epoch : 3 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.941488\n",
      "Train Epoch : 3 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.334090\n",
      "Train Epoch : 3 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.301407\n",
      "\n",
      "[Epoch : 3], \t Test Loss : 0.0200, \t Test Accuracy : 54.58 %\n",
      "\n",
      "Train Epoch : 4 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.491397\n",
      "Train Epoch : 4 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.095342\n",
      "Train Epoch : 4 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.207798\n",
      "Train Epoch : 4 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.182075\n",
      "\n",
      "[Epoch : 4], \t Test Loss : 0.0189, \t Test Accuracy : 57.96 %\n",
      "\n",
      "Train Epoch : 5 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.175173\n",
      "Train Epoch : 5 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.249971\n",
      "Train Epoch : 5 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.044832\n",
      "Train Epoch : 5 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.004305\n",
      "\n",
      "[Epoch : 5], \t Test Loss : 0.0188, \t Test Accuracy : 58.05 %\n",
      "\n",
      "Train Epoch : 6 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.060480\n",
      "Train Epoch : 6 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.047376\n",
      "Train Epoch : 6 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.102576\n",
      "Train Epoch : 6 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.173543\n",
      "\n",
      "[Epoch : 6], \t Test Loss : 0.0195, \t Test Accuracy : 55.82 %\n",
      "\n",
      "Train Epoch : 7 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.319849\n",
      "Train Epoch : 7 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.302040\n",
      "Train Epoch : 7 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.033276\n",
      "Train Epoch : 7 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.189880\n",
      "\n",
      "[Epoch : 7], \t Test Loss : 0.0195, \t Test Accuracy : 57.08 %\n",
      "\n",
      "Train Epoch : 8 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.018079\n",
      "Train Epoch : 8 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.127288\n",
      "Train Epoch : 8 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.393265\n",
      "Train Epoch : 8 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.996127\n",
      "\n",
      "[Epoch : 8], \t Test Loss : 0.0188, \t Test Accuracy : 58.53 %\n",
      "\n",
      "Train Epoch : 9 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.145976\n",
      "Train Epoch : 9 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.125866\n",
      "Train Epoch : 9 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.014904\n",
      "Train Epoch : 9 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.157380\n",
      "\n",
      "[Epoch : 9], \t Test Loss : 0.0190, \t Test Accuracy : 59.01 %\n",
      "\n",
      "Train Epoch : 10 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.095868\n",
      "Train Epoch : 10 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.289395\n",
      "Train Epoch : 10 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.175841\n",
      "Train Epoch : 10 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.860816\n",
      "\n",
      "[Epoch : 10], \t Test Loss : 0.0191, \t Test Accuracy : 57.33 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Epoch in range(1, Epochs+1):\n",
    "    train(model, train_loader, optimizer, 200)\n",
    "    test_loss, test_accracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[Epoch : {}], \\t Test Loss : {:.4f}, \\t Test Accuracy : {:.2f} %\\n\".format(Epoch, test_loss, test_accracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3a6afb4ca1356324ac719a8f4904d3d9af325fac862a66ccc73b4ecb2536197"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('torch1.7': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
