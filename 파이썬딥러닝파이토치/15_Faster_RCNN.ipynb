{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch를 사용할 때 필요한 판다스, 넘파이, glob, matplotlib 등 다양한 모듈들을 빠르게 불러와준다.\n",
    "from torch_snippets import *\n",
    "# 이미지 분석 및 처리를 쉽게 할 수 있게 해주는 라이브러리 \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_ROOT = './dataset/images/images'\n",
    "df_raw = df = pd.read_csv('./dataset/df.csv')\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target2label : {1: 'Bus', 2: 'Truck', 0: 'background'}\n",
      "label2target : {'Bus': 1, 'Truck': 2, 'background': 0}\n"
     ]
    }
   ],
   "source": [
    "label2target = {l : t + 1 for t,l in enumerate(df_raw['LabelName'].unique())}\n",
    "label2target['background'] = 0\n",
    "target2label = {t : l for l,t in label2target.items()}\n",
    "background_class = label2target['background']\n",
    "num_classes = len(label2target)\n",
    "print('target2label : {}'.format(target2label))\n",
    "print('label2target : {}'.format(label2target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_image(img):\n",
    "    # permute tensor의 차원을 바꿔준다. 2,0,1 차원에 있던 값들을 0,1,2 차원에 순서대로 넣어준다.\n",
    "    # w,h,chennels 순으로 들어와 있기 때문에 Channels,w,h 순으로 바꿔준다.\n",
    "    img = torch.tensor(img).permute(2,0,1)\n",
    "    return img.to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenDataset(torch.utils.data.Dataset):\n",
    "    # 다양한 이미지 크기를 224 * 224로 바꿔주기 위해 속성값 넣어주기\n",
    "    w,h = 224, 224\n",
    "    def __init__(self,df,image_dir = IMAGE_ROOT):\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        # 이미지 경로 + /* 를 해줘서 glob의 input형태를 맞춰준다.\n",
    "        self.files = glob.glob(self.image_dir + '/*')\n",
    "\n",
    "        # image_infos 에 데이터 프레임의 ImageID의 중복되지않은 값들을 넣어준다.\n",
    "        # 15225개, 넘파이 array로 들어간다.\n",
    "        self.image_infos = df.ImageID.unique()\n",
    "    \n",
    "    def __getitem__(self,ix):\n",
    "        # image_infos 에서 인덱스로 접근해 image_id에 담아준다.\n",
    "        image_id = self.image_infos[ix]\n",
    "        \n",
    "        # torch_snippets를 사용해 사용할 수 있는 find는 image_id 에 담겨있는 것들이 self.files에 포함되어 있다면 출력해준다.\n",
    "        # print(find('asd', ['qweasd','qweacs','xzcasdc','qwecxn]))  =>  ['qweasd','xzcasdc']\n",
    "        img_path = find(image_id,self.files)\n",
    "\n",
    "        # 이미지를 열 때, BGR 로 열리는 이미지를 RGB로 열어준다.\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # 이미지를 resize 하는데 객체 속성 값인 224 * 224 로 해준다. \n",
    "        img = np.array(img.resize((self.w,self.h), resample=Image.BILINEAR)) / 255.\n",
    "\n",
    "        # image_id 와 df['ImageID']에서 동일한 값만 갖는 내용을 dataframe에서 뽑아온다.\n",
    "        data = df[df['ImageID'] == image_id]\n",
    "        # 뽑아온 data에서 LabelName 에 해당하는 값들만 리스트로 담아 labels에 저장한다.\n",
    "        labels = data['LabelName'].values.tolist()\n",
    "\n",
    "        # 뽑아온 data에서 XMin, YMin, XMax, YMax 의 값들을 data에 다시 초기화 시켜준다.\n",
    "        data = data[['XMin','YMin','XMax','YMax']].values\n",
    "        \n",
    "        # 절대 좌표로 변환\n",
    "        # data에는 [[Xmin, Ymin, Xmax, Ymax],[Xmin, Ymin, Xmax, Ymax],[Xmin, Ymin, Xmax, Ymax]...] 값들이 순서대로 들어가있는 상태이다.\n",
    "        # [:,0,2] = [[Xmin, Xmax],[Xmin,Xmax]...] 에 224의 값을 모두 곱해준다.\n",
    "        # [:,1,3] = [Ymin, Ymax],[Ymin,Ymax]...] 에 224의 값을 모두 곱해준다.\n",
    "        data[:,[0,2]] *= self.w\n",
    "        data[:,[1,3]] *= self.h\n",
    "        # boxes 변수에 dat를 담아주는데 모두 양수의 값이기 때문에 uint32를 자료형으로 변형해고 리스트에 담아 저장해준다.        \n",
    "        boxes = data.astype(np.uint32).tolist()\n",
    "\n",
    "        # pytorch Faster- RCNN 모델은 텐서의 dictionary type 으로 ground truth를 받는다.\n",
    "        # ground truth 는 우리가 탐지하고자 하는 객체의 위치를 labeling 한 값을 의미한다.\n",
    "        target = {}\n",
    "        target['boxes'] = torch.Tensor(boxes).float()\n",
    "        target['labels'] = torch.Tensor([label2target[i] for i in labels]).long()\n",
    "        print(target)\n",
    "        \n",
    "        # 위에서 정의한 preprocessing_image 함수를 통해 이미지를 받아온다.\n",
    "        img = preprocessing_image(img)\n",
    "        return img, target\n",
    "\n",
    "    # 보통 torch 의 tensor를 받지만 여기서는 dict 타입을 받는다.\n",
    "    def collate_fn(self,batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    # 길이를 출력하면 df['ImageID']의 unique 값의 길이가 출력된다.\n",
    "    def __len__(self):\n",
    "        return len(self.image_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반가운 sklearn train_test_split\n",
    "# dataframe에서 ImageID의 유니크 값들 중 train_size = 90%, test_size = 10%, Random_State 값을 99로 맞춰주고 trn_ids, val_ids로 나눠 담는다. \n",
    "from sklearn.model_selection import train_test_split\n",
    "trn_ids, val_ids = train_test_split(df['ImageID'].unique(),test_size=0.1, random_state=99)\n",
    "# dataframe의 ImageID 에서 trn_dis가 있는 행들만 trn_df, val_ids가 있는 행들만 val_df에 담는다.\n",
    "trn_df, val_df = df[df['ImageID'].isin(trn_ids)], df[df['ImageID'].isin(val_ids)]\n",
    "\n",
    "# 위에서 정의한 OpenDataset class에 df = trn_df ,image_dir 는 default 값으로 넣어 훈련, 검증용 인스턴스를 각각 정의한다.\n",
    "train_ds = OpenDataset(trn_df)\n",
    "test_ds = OpenDataset(val_df)\n",
    "print('훈련용 dataset 크기 : {}\\n검증용 dataset 크기 : {}'.format(len(train_ds),len(test_ds)))\n",
    "print('이미지 파일 경로 : {}'.format(train_ds.image_dir))\n",
    "# print('정보가 저장되어 있는 데이터 프레임 : {}'.format(train_ds.df))\n",
    "print('이미지 파일의 이름들 : {}'.format(train_ds.image_infos))\n",
    "\n",
    "# train_ds를 넣어주면 OpenDataset Class의 __getitem__ method에 있는 return 값들이 index값에 따라 반환된다. \n",
    "# batch_size = 4\n",
    "# batch sample로 묶인 이후에는 collate_fn을 호출해 묶어준다.\n",
    "# drop_last 는 배치사이즈에 맞지 않는 마지막데이터는 버린다는 의미이다.\n",
    "train_loader = DataLoader(train_ds, batch_size = 4, collate_fn=train_ds.collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, collate_fn=test_ds.collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vision 과 관련된 torch의 모듈 불러오기\n",
    "import torchvision\n",
    "\n",
    "# torchvision 에서 제공하는 detection 모델 중 faster rcnn 불어오는데 Backbone은 resnet50으로 한다.\n",
    "# Backbone 은 일반적인 CNN을 거쳐 평범하게 feature map을 생성하는 파트\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "def get_model():\n",
    "    # FPN(Feature Pyramid Network)을 통해 컴퓨팅 자원을 적게 차지하면서 다양한 크기의 객체를 인식하는 방법이다. 자세한 내용은 구글링\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련\n",
    "def train_batch(inputs, model, optimizer):\n",
    "    # model 훈련 모드\n",
    "    model.train()\n",
    "\n",
    "    # inputs 으로 우리는 train_loader 를 넣을건데 train_loader 는 input과 target 으로 나뉜다.\n",
    "    input, targets = inputs\n",
    "\n",
    "    # input 에서 이미지를 받아오고 이미지를 device 에 할당하고 이후 리스트에 담아 input에 초기화 시켜준다.\n",
    "    input = list(image.to(device) for image in input)\n",
    "    \n",
    "    # torchvision의 faster rcnn 에서는 label과 box 값을 dict type으로 받는다.\n",
    "    targets = [{k : v.to(device) for k,v in t.items()} for t in targets]\n",
    "    \n",
    "    # 훈련 이후 optimizer에 있는 기울기 값들을 모두 0으로 바꿔준다.(추후 연산에서 영향을 주지 않기 위해)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # model에 input과 targets 값을 넣어준다.\n",
    "    losses = model(input, targets)\n",
    "\n",
    "    # losses에 있는 값들을 모두 합쳐준다.\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "\n",
    "    # loss 값에 대해 backpropagation 을 진행\n",
    "    loss.backward()\n",
    "\n",
    "    # 이후 최적화 진행\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss , losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_batch(inputs, model):\n",
    "    # losses 값을 얻기 위해서는 반드시 model.train() 에서 가능하다.\n",
    "    model.train()\n",
    "    input, targets = inputs\n",
    "    input = list(image.to(device) for image in inputs)\n",
    "    targets = [{k : v.to(device) for k,v in t.items()} for t in targets]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    return loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "n_epochs = 5\n",
    "log = Report(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1.000\ttrn_loss: 6965319505318620.000\ttrn_loc_loss: 4171984631618.364\ttrn_regr_loss: 6809436177930.483\ttrn_objectness_loss: 2113611395476664.750\ttrn_rpn_box_reg_loss: 4840726807765855.000\tval_loss: 3363144253.129\tval_loc_loss: 0.692\tval_regr_loss: 0.000\tval_objectness_loss: 918142314.807\tval_rpn_box_reg_loss: 2445001927.472\t(9999.23s - 39996.90s remaining)\n",
      "EPOCH: 2.000\ttrn_loss: 3106346859.828\ttrn_loc_loss: 1224.960\ttrn_regr_loss: 1087.151\ttrn_objectness_loss: 1022005726.123\ttrn_rpn_box_reg_loss: 2084338824.623\tval_loss: 650361027.221\tval_loc_loss: 0.692\tval_regr_loss: 0.000\tval_objectness_loss: 162494822.632\tval_rpn_box_reg_loss: 487866203.537\t(11639.63s - 17459.44s remaining)\n",
      "EPOCH: 3.000\ttrn_loss: 44673450187030.945\ttrn_loc_loss: 16562.045\ttrn_regr_loss: 25831.000\ttrn_objectness_loss: 8958142131540.887\ttrn_rpn_box_reg_loss: 35715309070365.531\tval_loss: 3541927569.347\tval_loc_loss: 0.692\tval_regr_loss: 0.000\tval_objectness_loss: 685724097.705\tval_rpn_box_reg_loss: 2856203457.768\t(13281.00s - 8854.00s remaining)\n",
      "EPOCH: 4.000\ttrn_loss: 3972712458.676\ttrn_loc_loss: 67.808\ttrn_regr_loss: 90.626\ttrn_objectness_loss: 1638977076.355\ttrn_rpn_box_reg_loss: 2333735213.355\tval_loss: 225616357.689\tval_loc_loss: 0.692\tval_regr_loss: 0.000\tval_objectness_loss: 59105862.954\tval_rpn_box_reg_loss: 166510493.531\t(14887.41s - 3721.85s remaining)\n",
      "EPOCH: 5.000\ttrn_loss: 359426665236698944.000\ttrn_loc_loss: 0.694\ttrn_regr_loss: 5649.944\ttrn_objectness_loss: 109313392940859456.000\ttrn_rpn_box_reg_loss: 250113269732035456.000\tval_loss: 193159196.968\tval_loc_loss: 0.692\tval_regr_loss: 0.000\tval_objectness_loss: 50129020.347\tval_rpn_box_reg_loss: 143030176.805\t(16253.70s - 0.00s remaining)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    _n = len(train_loader)\n",
    "    for ix, inputs in enumerate(train_loader):\n",
    "        loss, losses = train_batch(inputs, model, optimizer)\n",
    "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in ['loss_classifier',\n",
    "                                                                                      'loss_box_reg',\n",
    "                                                                                      'loss_objectness',\n",
    "                                                                                      'loss_rpn_box_reg']]\n",
    "        pos = (epoch + (ix+1) / _n)\n",
    "        log.record(pos, trn_loss = loss.item(), \n",
    "                        trn_loc_loss = loc_loss.item(),\n",
    "                        trn_regr_loss = regr_loss.item(), \n",
    "                        trn_objectness_loss = loss_objectness.item(),\n",
    "                        trn_rpn_box_reg_loss = loss_rpn_box_reg.item(),\n",
    "                        end = '\\r')\n",
    "    \n",
    "    _n = len(test_loader)\n",
    "\n",
    "    for ix, inputs in enumerate(test_loader):\n",
    "        loss, losses = validate_batch(inputs, model)\n",
    "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in ['loss_classifier',\n",
    "                                                                                      'loss_box_reg',\n",
    "                                                                                      'loss_objectness',\n",
    "                                                                                      'loss_rpn_box_reg']]\n",
    "        pos = (epoch + (ix+1) / _n)\n",
    "        log.record(pos, val_loss = loss.item(), \n",
    "                        val_loc_loss = loc_loss.item(),\n",
    "                        val_regr_loss = regr_loss.item(), \n",
    "                        val_objectness_loss = loss_objectness.item(),\n",
    "                        val_rpn_box_reg_loss = loss_rpn_box_reg.item(),\n",
    "                        end = '\\r')\n",
    "\n",
    "    if (epoch + 1) % (n_epochs // 5) == 0:\n",
    "        log.report_avgs(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:00<00:00, 114.84it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log.plot_epochs(['trn_loss','val_loss'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "277261f543cd8072db14e00033800a710bcf12c57ced4b367001bb0f6e57b623"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch1.7': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
