{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version : 1.7.1+cu110\n",
      "torch device : cuda\n",
      "Batch size : 64\n",
      "Epochs : 10\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "BATCH_SIZE = 64\n",
    "Epochs = 10\n",
    "print('torch version : {}'.format(torch.__version__))\n",
    "print('torch device : {}'.format(DEVICE))\n",
    "print('Batch size : {}\\nEpochs : {}'.format(BATCH_SIZE, Epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_datasets = datasets.CIFAR10(root = './Data/CIFAR_10',\n",
    "                                  train=True,\n",
    "                                  download=True,\n",
    "                                  # ToTensor 가 아닌 Compose를 이용해 Augmentation 을 바로 진행할 수 있다.\n",
    "                                  transform = transforms.Compose([\n",
    "                                      # 50% 확률로 좌우반전\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      # 0에서 1사이의 값으로 정규화, Tensor의 형태로 변환\n",
    "                                      transforms.ToTensor(),\n",
    "                                      # 다시 한 번 정규화를 진행 red, green, blue 순으로 0.5의 평균, 표준편차로 적용\n",
    "                                      transforms.Normalize((0.5,0.5,0.5),\n",
    "                                      (0.5, 0.5, 0.5))\n",
    "                                  ]))\n",
    "test_datasets = datasets.CIFAR10(root = './Data/CIFAR_10',\n",
    "                                  train=False,\n",
    "                                  download=True,\n",
    "                                  transform = transforms.Compose([\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5),\n",
    "                                      (0.5, 0.5, 0.5))\n",
    "                                  ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_datasets, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_datasets, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train 의 size : torch.Size([64, 3, 32, 32])\n",
      "y_train 의 size : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for (x_train, y_train) in train_loader:\n",
    "    print(\"x_train 의 size : {}\".format(x_train.size()))\n",
    "    print(\"y_train 의 size : {}\".format(y_train.size()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,in_planes,planes,stride = 1):\n",
    "        super(BasicBlock,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes,planes,kernel_size=3,stride=stride,padding=1,bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes,planes,kernel_size=3,stride=1,padding=1,bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # short_cut 의 역할은 stride가 달라 차원이 달라진 경우, kernel_size는 1로 고정하고 stride값을 조정해 차원을 맞춰주는 역할이다.\n",
    "        self.short_cut = nn.Sequential()\n",
    "        if stride != 1:\n",
    "            self.short_cut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes,planes,kernel_size=1,stride=stride, bias = False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.short_cut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(ResNet18,self).__init__()\n",
    "        self.blocks = [2,2,2,2]\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3,64,kernel_size=3,stride=2,bias = False)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "\n",
    "        # layer0 의 stride 값이 1인 이유는 바로 위, MaxPool2d를 통해 stride값을 주며 이미 출력이미지에 맞춰서 넘어왔기 때문이다.\n",
    "        # 나머지 layer sequence 들은 다음 block으로 넘어갈 때 출력이미지가 절반으로 줄어들기 때문에 stride = 2 값을 준다.\n",
    "        self.layer1 = self.make_layer(64,self.blocks[0],stride=1)\n",
    "        self.layer2 = self.make_layer(128,self.blocks[1],stride=2)\n",
    "        self.layer3 = self.make_layer(256,self.blocks[2],stride=2)\n",
    "        self.layer4 = self.make_layer(512,self.blocks[3],stride=2)\n",
    "\n",
    "        self.fc_layer = nn.Linear(512,10)\n",
    "    \n",
    "    def make_layer(self,planes,block,stride):\n",
    "        strides = [stride] + block * [1]\n",
    "\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes,planes,stride))\n",
    "            self.in_planes = planes\n",
    "        \n",
    "        return nn.Sequential(* layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.pool1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        # Feature Map에 4*4 filter가 움직이면서 16개의 Feature Map 값의 평균을 계산해 1개의 Feature Map으로 다운샘플링\n",
    "        # out = F.avg_pool2d(out,8)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.fc_layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (short_cut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (fc_layer): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx,(image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch : {} [ {} / {} ( {:.0f}% ) ]'.format(Epoch, batch_idx * len(image), len(train_loader.dataset), 100 * batch_idx / len(train_loader)), end = ',  ')\n",
    "            print('Train Loss : {:.6f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output,label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy = 100 * correct / len(test_loader.dataset)\n",
    "\n",
    "        return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch : 1 [ 0 / 50000 ( 0% ) ],  Train Loss : 2.752886\n",
      "Train Epoch : 1 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.954313\n",
      "Train Epoch : 1 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.643712\n",
      "Train Epoch : 1 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.674519\n",
      "\n",
      "[Epoch : 1], \t Test Loss : 0.0251, \t Test Accuracy : 42.93 %\n",
      "\n",
      "Train Epoch : 2 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.599076\n",
      "Train Epoch : 2 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.339125\n",
      "Train Epoch : 2 [ 25600 / 50000 ( 51% ) ],  Train Loss : 1.214056\n",
      "Train Epoch : 2 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.265808\n",
      "\n",
      "[Epoch : 2], \t Test Loss : 0.0199, \t Test Accuracy : 54.69 %\n",
      "\n",
      "Train Epoch : 3 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.135648\n",
      "Train Epoch : 3 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.204255\n",
      "Train Epoch : 3 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.999050\n",
      "Train Epoch : 3 [ 38400 / 50000 ( 77% ) ],  Train Loss : 1.060992\n",
      "\n",
      "[Epoch : 3], \t Test Loss : 0.0176, \t Test Accuracy : 62.61 %\n",
      "\n",
      "Train Epoch : 4 [ 0 / 50000 ( 0% ) ],  Train Loss : 1.117838\n",
      "Train Epoch : 4 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.063531\n",
      "Train Epoch : 4 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.791577\n",
      "Train Epoch : 4 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.841680\n",
      "\n",
      "[Epoch : 4], \t Test Loss : 0.0148, \t Test Accuracy : 67.01 %\n",
      "\n",
      "Train Epoch : 5 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.788993\n",
      "Train Epoch : 5 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.068118\n",
      "Train Epoch : 5 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.772902\n",
      "Train Epoch : 5 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.836881\n",
      "\n",
      "[Epoch : 5], \t Test Loss : 0.0135, \t Test Accuracy : 70.43 %\n",
      "\n",
      "Train Epoch : 6 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.720974\n",
      "Train Epoch : 6 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.825110\n",
      "Train Epoch : 6 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.710473\n",
      "Train Epoch : 6 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.605440\n",
      "\n",
      "[Epoch : 6], \t Test Loss : 0.0129, \t Test Accuracy : 71.95 %\n",
      "\n",
      "Train Epoch : 7 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.639457\n",
      "Train Epoch : 7 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.673043\n",
      "Train Epoch : 7 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.728041\n",
      "Train Epoch : 7 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.806940\n",
      "\n",
      "[Epoch : 7], \t Test Loss : 0.0118, \t Test Accuracy : 74.18 %\n",
      "\n",
      "Train Epoch : 8 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.518625\n",
      "Train Epoch : 8 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.556695\n",
      "Train Epoch : 8 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.892685\n",
      "Train Epoch : 8 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.585121\n",
      "\n",
      "[Epoch : 8], \t Test Loss : 0.0118, \t Test Accuracy : 74.89 %\n",
      "\n",
      "Train Epoch : 9 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.636436\n",
      "Train Epoch : 9 [ 12800 / 50000 ( 26% ) ],  Train Loss : 1.005930\n",
      "Train Epoch : 9 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.341540\n",
      "Train Epoch : 9 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.552499\n",
      "\n",
      "[Epoch : 9], \t Test Loss : 0.0111, \t Test Accuracy : 76.17 %\n",
      "\n",
      "Train Epoch : 10 [ 0 / 50000 ( 0% ) ],  Train Loss : 0.504620\n",
      "Train Epoch : 10 [ 12800 / 50000 ( 26% ) ],  Train Loss : 0.483607\n",
      "Train Epoch : 10 [ 25600 / 50000 ( 51% ) ],  Train Loss : 0.589598\n",
      "Train Epoch : 10 [ 38400 / 50000 ( 77% ) ],  Train Loss : 0.459772\n",
      "\n",
      "[Epoch : 10], \t Test Loss : 0.0112, \t Test Accuracy : 76.07 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Epoch in range(1, Epochs+1):\n",
    "    train(model, train_loader, optimizer, 200)\n",
    "    test_loss, test_accracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[Epoch : {}], \\t Test Loss : {:.4f}, \\t Test Accuracy : {:.2f} %\\n\".format(Epoch, test_loss, test_accracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3a6afb4ca1356324ac719a8f4904d3d9af325fac862a66ccc73b4ecb2536197"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('torch1.7': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
